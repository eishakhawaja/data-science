# -*- coding: utf-8 -*-
"""assignment4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P0DOWVM2NK_EbUDAJ2psJQQPgEmUV5ve
"""

# Date 11/12/2023
# CSC461 – Assignment4 – NLP
# Eisha Fatima Rashid
# FA21-BSE-084
# calculating the termfrequency , inverse document frequency and combining both term and invetse document frequency in tables respectively.As well as calculating their similarity measure , manhattan and eucleadean distance

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')



# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity, manhattan_distances, euclidean_distances
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Sample text data
text1 = "Data science is important for computer science."
text2 = "This is one of the best data science courses."
text3 = "Data scientists analyze data."

# Set up NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

# Define a simple tokenizer function
def tokenize(text):
    # Tokenize the text
    words = nltk.word_tokenize(text)
    # Remove common words (stop words) and simplify words (lemmatization)
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return words

# Create a CountVectorizer to compute BoW
cv = CountVectorizer(tokenizer=tokenize)
bow_matrix = cv.fit_transform([text1, text2, text3])

# Create a table for Bag of Words (BoW)
bow_table = pd.DataFrame(bow_matrix.toarray(), index=["Text 1", "Text 2", "Text 3"], columns=cv.get_feature_names_out())

# Create a TF-IDF transformer
tfidf_transformer = TfidfTransformer()

# Compute TF-IDF
tfidf_matrix = tfidf_transformer.fit_transform(bow_matrix)

# Create a table for TF-IDF
tfidf_table = pd.DataFrame(tfidf_matrix.toarray(), index=["Text 1", "Text 2", "Text 3"], columns=cv.get_feature_names_out())

# Create a CountVectorizer to compute Term Frequency (TF)
tf_vectorizer = CountVectorizer(tokenizer=tokenize)
tf_matrix = tf_vectorizer.fit_transform([text1, text2, text3])

# Create a table for Term Frequency (TF)
tf_table = pd.DataFrame(tf_matrix.toarray(), index=["Text 1", "Text 2", "Text 3"], columns=tf_vectorizer.get_feature_names_out())

# Compute Inverse Document Frequency (IDF)
idf_values = tfidf_transformer.idf_

# Create a table for IDF
idf_table = pd.DataFrame(idf_values, index=tf_vectorizer.get_feature_names_out(), columns=["IDF"])

# Calculate cosine similarity between texts
cosine_sim = cosine_similarity(tfidf_matrix)

# Create a table for Cosine Similarity
cosine_sim_table = pd.DataFrame(cosine_sim, index=["Text 1", "Text 2", "Text 3"], columns=["Text 1", "Text 2", "Text 3"])

# Calculate Manhattan distance between texts
manhattan_dist = manhattan_distances(tfidf_matrix)

# Create a table for Manhattan Distance
manhattan_dist_table = pd.DataFrame(manhattan_dist, index=["Text 1", "Text 2", "Text 3"], columns=["Text 1", "Text 2", "Text 3"])

# Calculate Euclidean distance between texts
euclidean_dist = euclidean_distances(tfidf_matrix)

# Create a table for Euclidean Distance
euclidean_dist_table = pd.DataFrame(euclidean_dist, index=["Text 1", "Text 2", "Text 3"], columns=["Text 1", "Text 2", "Text 3"])

# Display the BoW, TF, IDF, TF-IDF, Cosine Similarity, Manhattan Distance, and Euclidean Distance tables
print("Bag of Words (BoW) Table:")
print(bow_table)
print("\nTerm Frequency (TF) Table:")
print(tf_table)
print("\nInverse Document Frequency (IDF) Table:")
print(idf_table)
print("\nTF-IDF Table:")
print(tfidf_table)
print("\nCosine Similarity Table:")
print(cosine_sim_table)
print("\nManhattan Distance Table:")
print(manhattan_dist_table)
print("\nEuclidean Distance Table:")
print(euclidean_dist_table)